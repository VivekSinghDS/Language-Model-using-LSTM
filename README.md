# Language Model using LSTM

For data, I took the text data from the compiled versions of all the Harry Potter books, and then generated tokens of all this data. After this, we create a network of LSTM, onto which we feed this data and then generate the next possible tokens. 

PS : The model is trained for 100 epochs

Examples : 

Input : Harry Has Been Wandering


Output : Harry Has Been Wandering Over To The Hospital Wing For A Moment He Had To Get A Word From

Time taken : 500ms 


Other examples are also in the notebook, and the dependencies are also shown in the notebook named "lang_model_lstm.ipynb"




   
