# Language Model using LSTM

For data, I took the text data from the compiled versions of all the Harry Potter books, and then generated tokens of all this data. After this, we create a network of LSTM, onto which we feed this data and then generate the next possible tokens. 

PS : I have provided a model just in case if you want to skip training :), and is trained for 100 epochs



Examples : 

Input : Harry Has Been Wandering


Output : Harry Has Been Wandering Over To The Hospital Wing For A Moment He Had To Get A Word From

Time taken : 500ms 


Optimized code is present where one can just load the data and process their outcome. 






   
